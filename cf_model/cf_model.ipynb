{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4094b2b9",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "This notebook builds a matrix factorization-based collaborative filtering model with TensorFlow using the Netflix ratings data stored in the `../data` directory. We will prepare the data, train an embedding model with implicit regularization, and surface sample movie recommendations for an arbitrary user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024de185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a913f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 20,000,263 rows, 138,493 users, 26,744 movies\n",
      "Movies metadata: 27,278 titles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:32:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:29:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating            timestamp\n",
       "0       1        2     3.5  2005-04-02 23:53:47\n",
       "1       1       29     3.5  2005-04-02 23:31:16\n",
       "2       1       32     3.5  2005-04-02 23:33:39\n",
       "3       1       47     3.5  2005-04-02 23:32:07\n",
       "4       1       50     3.5  2005-04-02 23:29:40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"..\").resolve() / \"data\"\n",
    "assert DATA_DIR.exists(), f\"Data directory not found at {DATA_DIR}\"\n",
    "\n",
    "ratings = pd.read_csv(DATA_DIR / \"rating.csv\")\n",
    "movies = pd.read_csv(DATA_DIR / \"movie.csv\")\n",
    "\n",
    "print(f\"Ratings: {ratings.shape[0]:,} rows, {ratings['userId'].nunique():,} users, {ratings['movieId'].nunique():,} movies\")\n",
    "print(f\"Movies metadata: {movies.shape[0]:,} titles\")\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55050d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.000026e+07\n",
      "mean     3.525529e+00\n",
      "std      1.051989e+00\n",
      "min      5.000000e-01\n",
      "25%      3.000000e+00\n",
      "50%      3.500000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "Ratings per user:\n",
      "count    138493.000000\n",
      "mean        144.413530\n",
      "std         230.267257\n",
      "min          20.000000\n",
      "25%          35.000000\n",
      "50%          68.000000\n",
      "75%         155.000000\n",
      "max        9254.000000\n",
      "dtype: float64\n",
      "\n",
      "Ratings per movie:\n",
      "count    26744.000000\n",
      "mean       747.841123\n",
      "std       3085.818268\n",
      "min          1.000000\n",
      "25%          3.000000\n",
      "50%         18.000000\n",
      "75%        205.000000\n",
      "max      67310.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rating_stats = ratings['rating'].describe()\n",
    "print(rating_stats)\n",
    "\n",
    "ratings_per_user = ratings.groupby('userId').size().describe()\n",
    "print(\"\\nRatings per user:\")\n",
    "print(ratings_per_user)\n",
    "\n",
    "ratings_per_item = ratings.groupby('movieId').size().describe()\n",
    "print(\"\\nRatings per movie:\")\n",
    "print(ratings_per_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c992bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will train on 138493 users and 26744 items\n"
     ]
    }
   ],
   "source": [
    "def build_id_mappings(values: pd.Series):\n",
    "    unique_values = np.sort(values.unique())\n",
    "    value_to_idx = {value: idx for idx, value in enumerate(unique_values)}\n",
    "    idx_to_value = {idx: value for value, idx in value_to_idx.items()}\n",
    "    return value_to_idx, idx_to_value\n",
    "\n",
    "user_to_idx, idx_to_user = build_id_mappings(ratings['userId'])\n",
    "movie_to_idx, idx_to_movie = build_id_mappings(ratings['movieId'])\n",
    "\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx).astype(np.int32)\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx).astype(np.int32)\n",
    "\n",
    "num_users = len(user_to_idx)\n",
    "num_items = len(movie_to_idx)\n",
    "print(f\"Model will train on {num_users} users and {num_items} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710d40b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000236, 2000027)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    ratings,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "SHUFFLE_BUFFER = 1_000_000\n",
    "\n",
    "\n",
    "def df_to_dataset(df: pd.DataFrame, training: bool = True) -> tf.data.Dataset:\n",
    "    features = {\n",
    "        \"user_id\": df['user_idx'].values.astype(np.int32),\n",
    "        \"item_id\": df['movie_idx'].values.astype(np.int32),\n",
    "    }\n",
    "    labels = df['rating'].values.astype(np.float32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(min(len(df), SHUFFLE_BUFFER), seed=SEED, reshuffle_each_iteration=True)\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_ds = df_to_dataset(train_df, training=True)\n",
    "val_ds = df_to_dataset(val_df, training=False)\n",
    "\n",
    "len(train_df), len(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45241cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    \"\"\"Simple dot-product collaborative filtering with user/item biases.\"\"\"\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int, embedding_dim: int = 64, reg: float = 1e-6, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.reg = reg\n",
    "        \n",
    "        regularizer = tf.keras.regularizers.l2(reg)\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_users,\n",
    "            output_dim=embedding_dim,\n",
    "            embeddings_regularizer=regularizer,\n",
    "            name=\"user_embedding\"\n",
    "        )\n",
    "        self.item_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_items,\n",
    "            output_dim=embedding_dim,\n",
    "            embeddings_regularizer=regularizer,\n",
    "            name=\"item_embedding\"\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1, name=\"user_bias\")\n",
    "        self.item_bias = tf.keras.layers.Embedding(num_items, 1, name=\"item_bias\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vec = self.user_embedding(inputs[\"user_id\"])\n",
    "        item_vec = self.item_embedding(inputs[\"item_id\"])\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "        user_b = tf.squeeze(self.user_bias(inputs[\"user_id\"]), axis=1)\n",
    "        item_b = tf.squeeze(self.item_bias(inputs[\"item_id\"]), axis=1)\n",
    "        return dot_product + user_b + item_b\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_users\": self.num_users,\n",
    "            \"num_items\": self.num_items,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"reg\": self.reg,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da851b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4395/4395 - 278s - 63ms/step - loss: 2.2908 - rmse: 1.4648 - val_loss: 0.9654 - val_rmse: 0.8776\n",
      "Epoch 2/20\n",
      "4395/4395 - 329s - 75ms/step - loss: 0.9222 - rmse: 0.8546 - val_loss: 0.8984 - val_rmse: 0.8439\n",
      "Epoch 3/20\n",
      "4395/4395 - 349s - 80ms/step - loss: 0.8560 - rmse: 0.8220 - val_loss: 0.8521 - val_rmse: 0.8228\n",
      "Epoch 4/20\n",
      "4395/4395 - 334s - 76ms/step - loss: 0.8008 - rmse: 0.7936 - val_loss: 0.8182 - val_rmse: 0.8068\n",
      "Epoch 5/20\n",
      "4395/4395 - 295s - 67ms/step - loss: 0.7529 - rmse: 0.7669 - val_loss: 0.7964 - val_rmse: 0.7963\n",
      "Epoch 6/20\n",
      "4395/4395 - 268s - 61ms/step - loss: 0.7110 - rmse: 0.7420 - val_loss: 0.7847 - val_rmse: 0.7913\n",
      "Epoch 7/20\n",
      "4395/4395 - 247s - 56ms/step - loss: 0.6763 - rmse: 0.7210 - val_loss: 0.7787 - val_rmse: 0.7903\n",
      "Epoch 8/20\n",
      "4395/4395 - 250s - 57ms/step - loss: 0.6493 - rmse: 0.7055 - val_loss: 0.7743 - val_rmse: 0.7910\n",
      "Epoch 9/20\n",
      "4395/4395 - 254s - 58ms/step - loss: 0.6282 - rmse: 0.6949 - val_loss: 0.7689 - val_rmse: 0.7918\n",
      "Epoch 10/20\n",
      "4395/4395 - 252s - 57ms/step - loss: 0.6114 - rmse: 0.6875 - val_loss: 0.7634 - val_rmse: 0.7924\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(num_users=num_users, num_items=num_items, embedding_dim=64, reg=1e-6)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")]\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_rmse\",\n",
    "    mode=\"min\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=20,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d84f90c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m eval_metrics = \u001b[43mmodel\u001b[49m.evaluate(val_ds, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[32m0\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m({k: \u001b[38;5;28mround\u001b[39m(v, \u001b[32m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_metrics.items()})\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(val_ds, return_dict=True, verbose=0)\n",
    "print({k: round(v, 4) for k, v in eval_metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d25c05f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m movie_titles = \u001b[43mmovies\u001b[49m.set_index(\u001b[33m'\u001b[39m\u001b[33mmovieId\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m].to_dict()\n\u001b[32m      2\u001b[39m user_consumed = ratings.groupby(\u001b[33m'\u001b[39m\u001b[33muser_idx\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33mmovie_idx\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28mset\u001b[39m).to_dict()\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_movies\u001b[39m(raw_user_id: \u001b[38;5;28mint\u001b[39m, top_k: \u001b[38;5;28mint\u001b[39m = \u001b[32m10\u001b[39m) -> pd.DataFrame:\n",
      "\u001b[31mNameError\u001b[39m: name 'movies' is not defined"
     ]
    }
   ],
   "source": [
    "movie_titles = movies.set_index('movieId')['title'].to_dict()\n",
    "user_consumed = ratings.groupby('user_idx')['movie_idx'].apply(set).to_dict()\n",
    "\n",
    "\n",
    "def recommend_movies(raw_user_id: int, top_k: int = 10) -> pd.DataFrame:\n",
    "    if raw_user_id not in user_to_idx:\n",
    "        raise ValueError(f\"User {raw_user_id} not found in the dataset\")\n",
    "\n",
    "    user_idx = user_to_idx[raw_user_id]\n",
    "    all_items = np.arange(num_items, dtype=np.int32)\n",
    "    user_vector = np.full_like(all_items, fill_value=user_idx)\n",
    "\n",
    "    preds = model({\n",
    "        \"user_id\": tf.convert_to_tensor(user_vector, dtype=tf.int32),\n",
    "        \"item_id\": tf.convert_to_tensor(all_items, dtype=tf.int32)\n",
    "    }).numpy()\n",
    "\n",
    "    seen_items = user_consumed.get(user_idx, set())\n",
    "    mask = np.ones_like(preds, dtype=bool)\n",
    "    mask[list(seen_items)] = False\n",
    "    filtered_scores = np.where(mask, preds, -np.inf)\n",
    "\n",
    "    top_indices = np.argpartition(filtered_scores, -top_k)[-top_k:]\n",
    "    top_indices = top_indices[np.argsort(filtered_scores[top_indices])[::-1]]\n",
    "\n",
    "    recommendations = []\n",
    "    for item_idx in top_indices:\n",
    "        movie_id = idx_to_movie[item_idx]\n",
    "        recommendations.append({\n",
    "            'movieId': movie_id,\n",
    "            'title': movie_titles.get(movie_id, 'Unknown'),\n",
    "            'score': float(filtered_scores[item_idx])\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "\n",
    "sample_user = ratings['userId'].iloc[1]\n",
    "recommend_movies(sample_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092525b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved model weights to saved_model\\model_weights.weights.h5\n",
      "✓ Saved model config to saved_model\\model_config.json\n",
      "✓ Saved mappings to saved_model\\mappings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Save model weights and configuration (works with already-trained model)\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "MODEL_DIR = Path(\"saved_model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(MODEL_DIR / \"model_weights.weights.h5\")\n",
    "print(f\"✓ Saved model weights to {MODEL_DIR / 'model_weights.weights.h5'}\")\n",
    "\n",
    "# Save model architecture config\n",
    "model_config = {\n",
    "    'num_users': num_users,\n",
    "    'num_items': num_items,\n",
    "    'embedding_dim': 64,\n",
    "    'reg': 1e-6\n",
    "}\n",
    "with open(MODEL_DIR / \"model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f)\n",
    "print(f\"✓ Saved model config to {MODEL_DIR / 'model_config.json'}\")\n",
    "\n",
    "# Save mappings for inference\n",
    "mappings = {\n",
    "    'user_to_idx': user_to_idx,\n",
    "    'idx_to_user': idx_to_user,\n",
    "    'movie_to_idx': movie_to_idx,\n",
    "    'idx_to_movie': idx_to_movie,\n",
    "    'movie_titles': movie_titles,\n",
    "    'user_consumed': user_consumed,\n",
    "    'num_users': num_users,\n",
    "    'num_items': num_items\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / \"mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mappings, f)\n",
    "print(f\"✓ Saved mappings to {MODEL_DIR / 'mappings.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d803964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved embeddings:\n",
      "  - User embeddings: (138493, 64)\n",
      "  - Item embeddings: (26744, 64)\n",
      "  → Use for approximate nearest neighbor search (FAISS, Annoy, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Option 3: Export embeddings only (for vector similarity search / ANN systems)\n",
    "# Extract learned embeddings for deployment to vector databases (Pinecone, Weaviate, etc.)\n",
    "\n",
    "user_embeddings = model.user_embedding.get_weights()[0]  # Shape: (num_users, embedding_dim)\n",
    "item_embeddings = model.item_embedding.get_weights()[0]  # Shape: (num_items, embedding_dim)\n",
    "user_biases = model.user_bias.get_weights()[0].flatten()\n",
    "item_biases = model.item_bias.get_weights()[0].flatten()\n",
    "\n",
    "np.save(MODEL_DIR / \"user_embeddings.npy\", user_embeddings)\n",
    "np.save(MODEL_DIR / \"item_embeddings.npy\", item_embeddings)\n",
    "np.save(MODEL_DIR / \"user_biases.npy\", user_biases)\n",
    "np.save(MODEL_DIR / \"item_biases.npy\", item_biases)\n",
    "\n",
    "print(f\"✓ Saved embeddings:\")\n",
    "print(f\"  - User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"  - Item embeddings: {item_embeddings.shape}\")\n",
    "print(\"  → Use for approximate nearest neighbor search (FAISS, Annoy, etc.)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
