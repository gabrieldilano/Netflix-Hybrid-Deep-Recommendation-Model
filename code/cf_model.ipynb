{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4094b2b9",
   "metadata": {},
   "source": [
    "# Collaborative Filtering\n",
    "\n",
    "This notebook builds a matrix factorization-based collaborative filtering model with TensorFlow using the Netflix ratings data stored in the `../data` directory. We will prepare the data, train an embedding model with implicit regularization, and surface sample movie recommendations for an arbitrary user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024de185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a913f3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings: 20,000,263 rows, 138,493 users, 26,744 movies\n",
      "Movies metadata: 27,278 titles\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:53:47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:31:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:33:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:32:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2005-04-02 23:29:40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating            timestamp\n",
       "0       1        2     3.5  2005-04-02 23:53:47\n",
       "1       1       29     3.5  2005-04-02 23:31:16\n",
       "2       1       32     3.5  2005-04-02 23:33:39\n",
       "3       1       47     3.5  2005-04-02 23:32:07\n",
       "4       1       50     3.5  2005-04-02 23:29:40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\"..\").resolve() / \"data\"\n",
    "assert DATA_DIR.exists(), f\"Data directory not found at {DATA_DIR}\"\n",
    "\n",
    "ratings = pd.read_csv(DATA_DIR / \"rating.csv\")\n",
    "movies = pd.read_csv(DATA_DIR / \"movie.csv\")\n",
    "\n",
    "print(\n",
    "    f\"Ratings: {ratings.shape[0]:,} rows, {ratings['userId'].nunique():,} users, {ratings['movieId'].nunique():,} movies\"\n",
    ")\n",
    "print(f\"Movies metadata: {movies.shape[0]:,} titles\")\n",
    "ratings.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55050d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    2.000026e+07\n",
      "mean     3.525529e+00\n",
      "std      1.051989e+00\n",
      "min      5.000000e-01\n",
      "25%      3.000000e+00\n",
      "50%      3.500000e+00\n",
      "75%      4.000000e+00\n",
      "max      5.000000e+00\n",
      "Name: rating, dtype: float64\n",
      "\n",
      "Ratings per user:\n",
      "count    138493.000000\n",
      "mean        144.413530\n",
      "std         230.267257\n",
      "min          20.000000\n",
      "25%          35.000000\n",
      "50%          68.000000\n",
      "75%         155.000000\n",
      "max        9254.000000\n",
      "dtype: float64\n",
      "\n",
      "Ratings per movie:\n",
      "count    26744.000000\n",
      "mean       747.841123\n",
      "std       3085.818268\n",
      "min          1.000000\n",
      "25%          3.000000\n",
      "50%         18.000000\n",
      "75%        205.000000\n",
      "max      67310.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "rating_stats = ratings[\"rating\"].describe()\n",
    "print(rating_stats)\n",
    "\n",
    "ratings_per_user = ratings.groupby(\"userId\").size().describe()\n",
    "print(\"\\nRatings per user:\")\n",
    "print(ratings_per_user)\n",
    "\n",
    "ratings_per_item = ratings.groupby(\"movieId\").size().describe()\n",
    "print(\"\\nRatings per movie:\")\n",
    "print(ratings_per_item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c992bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model will train on 138493 users and 26744 items\n"
     ]
    }
   ],
   "source": [
    "def build_id_mappings(values: pd.Series):\n",
    "    unique_values = np.sort(values.unique())\n",
    "    value_to_idx = {value: idx for idx, value in enumerate(unique_values)}\n",
    "    idx_to_value = {idx: value for value, idx in value_to_idx.items()}\n",
    "    return value_to_idx, idx_to_value\n",
    "\n",
    "\n",
    "user_to_idx, idx_to_user = build_id_mappings(ratings[\"userId\"])\n",
    "movie_to_idx, idx_to_movie = build_id_mappings(ratings[\"movieId\"])\n",
    "\n",
    "ratings[\"user_idx\"] = ratings[\"userId\"].map(user_to_idx).astype(np.int32)\n",
    "ratings[\"movie_idx\"] = ratings[\"movieId\"].map(movie_to_idx).astype(np.int32)\n",
    "\n",
    "num_users = len(user_to_idx)\n",
    "num_items = len(movie_to_idx)\n",
    "print(f\"Model will train on {num_users} users and {num_items} items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "710d40b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000236, 2000027)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    ratings, test_size=0.1, random_state=SEED, shuffle=True\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 4096\n",
    "SHUFFLE_BUFFER = 1_000_000\n",
    "\n",
    "\n",
    "def df_to_dataset(df: pd.DataFrame, training: bool = True) -> tf.data.Dataset:\n",
    "    features = {\n",
    "        \"user_id\": df[\"user_idx\"].values.astype(np.int32),\n",
    "        \"item_id\": df[\"movie_idx\"].values.astype(np.int32),\n",
    "    }\n",
    "    labels = df[\"rating\"].values.astype(np.float32)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        ds = ds.shuffle(\n",
    "            min(len(df), SHUFFLE_BUFFER), seed=SEED, reshuffle_each_iteration=True\n",
    "        )\n",
    "    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = df_to_dataset(train_df, training=True)\n",
    "val_ds = df_to_dataset(val_df, training=False)\n",
    "\n",
    "len(train_df), len(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45241cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(tf.keras.Model):\n",
    "    \"\"\"Simple dot-product collaborative filtering with user/item biases.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_users: int,\n",
    "        num_items: int,\n",
    "        embedding_dim: int = 64,\n",
    "        reg: float = 1e-6,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.reg = reg\n",
    "\n",
    "        regularizer = tf.keras.regularizers.l2(reg)\n",
    "        self.user_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_users,\n",
    "            output_dim=embedding_dim,\n",
    "            embeddings_regularizer=regularizer,\n",
    "            name=\"user_embedding\",\n",
    "        )\n",
    "        self.item_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=num_items,\n",
    "            output_dim=embedding_dim,\n",
    "            embeddings_regularizer=regularizer,\n",
    "            name=\"item_embedding\",\n",
    "        )\n",
    "        self.user_bias = tf.keras.layers.Embedding(num_users, 1, name=\"user_bias\")\n",
    "        self.item_bias = tf.keras.layers.Embedding(num_items, 1, name=\"item_bias\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_vec = self.user_embedding(inputs[\"user_id\"])\n",
    "        item_vec = self.item_embedding(inputs[\"item_id\"])\n",
    "        dot_product = tf.reduce_sum(user_vec * item_vec, axis=1)\n",
    "        user_b = tf.squeeze(self.user_bias(inputs[\"user_id\"]), axis=1)\n",
    "        item_b = tf.squeeze(self.item_bias(inputs[\"item_id\"]), axis=1)\n",
    "        return dot_product + user_b + item_b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"num_users\": self.num_users,\n",
    "                \"num_items\": self.num_items,\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"reg\": self.reg,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da851b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "4395/4395 - 215s - 49ms/step - loss: 2.3111 - rmse: 1.4723 - val_loss: 0.9608 - val_rmse: 0.8758\n",
      "Epoch 2/20\n",
      "4395/4395 - 234s - 53ms/step - loss: 0.9156 - rmse: 0.8508 - val_loss: 0.8924 - val_rmse: 0.8399\n",
      "Epoch 3/20\n",
      "4395/4395 - 227s - 52ms/step - loss: 0.8500 - rmse: 0.8177 - val_loss: 0.8487 - val_rmse: 0.8200\n",
      "Epoch 4/20\n",
      "4395/4395 - 250s - 57ms/step - loss: 0.7970 - rmse: 0.7905 - val_loss: 0.8171 - val_rmse: 0.8055\n",
      "Epoch 5/20\n",
      "4395/4395 - 250s - 57ms/step - loss: 0.7509 - rmse: 0.7652 - val_loss: 0.7962 - val_rmse: 0.7960\n",
      "Epoch 6/20\n",
      "4395/4395 - 244s - 56ms/step - loss: 0.7103 - rmse: 0.7415 - val_loss: 0.7845 - val_rmse: 0.7913\n",
      "Epoch 7/20\n",
      "4395/4395 - 249s - 57ms/step - loss: 0.6762 - rmse: 0.7211 - val_loss: 0.7784 - val_rmse: 0.7904\n",
      "Epoch 8/20\n",
      "4395/4395 - 243s - 55ms/step - loss: 0.6492 - rmse: 0.7058 - val_loss: 0.7740 - val_rmse: 0.7912\n",
      "Epoch 9/20\n",
      "4395/4395 - 243s - 55ms/step - loss: 0.6281 - rmse: 0.6951 - val_loss: 0.7690 - val_rmse: 0.7921\n",
      "Epoch 10/20\n",
      "4395/4395 - 246s - 56ms/step - loss: 0.6112 - rmse: 0.6877 - val_loss: 0.7636 - val_rmse: 0.7928\n"
     ]
    }
   ],
   "source": [
    "model = MatrixFactorization(\n",
    "    num_users=num_users, num_items=num_items, embedding_dim=64, reg=1e-6\n",
    ")\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=tf.keras.losses.MeanSquaredError(),\n",
    "    metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\")],\n",
    ")\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_rmse\", mode=\"min\", patience=3, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=20, callbacks=[early_stopping], verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d84f90c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7784, 'rmse': 0.7904}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(val_ds, return_dict=True, verbose=0)\n",
    "print({k: round(v, 4) for k, v in eval_metrics.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d25c05f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1210</td>\n",
       "      <td>Star Wars: Episode VI - Return of the Jedi (1983)</td>\n",
       "      <td>4.234172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34405</td>\n",
       "      <td>Serenity (2005)</td>\n",
       "      <td>4.157743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1197</td>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>4.155848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40815</td>\n",
       "      <td>Harry Potter and the Goblet of Fire (2005)</td>\n",
       "      <td>4.154024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1527</td>\n",
       "      <td>Fifth Element, The (1997)</td>\n",
       "      <td>4.140618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98809</td>\n",
       "      <td>Hobbit: An Unexpected Journey, The (2012)</td>\n",
       "      <td>4.127653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3578</td>\n",
       "      <td>Gladiator (2000)</td>\n",
       "      <td>4.118014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>480</td>\n",
       "      <td>Jurassic Park (1993)</td>\n",
       "      <td>4.112618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54259</td>\n",
       "      <td>Stardust (2007)</td>\n",
       "      <td>4.110387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>88125</td>\n",
       "      <td>Harry Potter and the Deathly Hallows: Part 2 (...</td>\n",
       "      <td>4.103004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                                              title     score\n",
       "0     1210  Star Wars: Episode VI - Return of the Jedi (1983)  4.234172\n",
       "1    34405                                    Serenity (2005)  4.157743\n",
       "2     1197                         Princess Bride, The (1987)  4.155848\n",
       "3    40815         Harry Potter and the Goblet of Fire (2005)  4.154024\n",
       "4     1527                          Fifth Element, The (1997)  4.140618\n",
       "5    98809          Hobbit: An Unexpected Journey, The (2012)  4.127653\n",
       "6     3578                                   Gladiator (2000)  4.118014\n",
       "7      480                               Jurassic Park (1993)  4.112618\n",
       "8    54259                                    Stardust (2007)  4.110387\n",
       "9    88125  Harry Potter and the Deathly Hallows: Part 2 (...  4.103004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_titles = movies.set_index(\"movieId\")[\"title\"].to_dict()\n",
    "user_consumed = ratings.groupby(\"user_idx\")[\"movie_idx\"].apply(set).to_dict()\n",
    "\n",
    "\n",
    "def recommend_movies(raw_user_id: int, top_k: int = 10) -> pd.DataFrame:\n",
    "    if raw_user_id not in user_to_idx:\n",
    "        raise ValueError(f\"User {raw_user_id} not found in the dataset\")\n",
    "\n",
    "    user_idx = user_to_idx[raw_user_id]\n",
    "    all_items = np.arange(num_items, dtype=np.int32)\n",
    "    user_vector = np.full_like(all_items, fill_value=user_idx)\n",
    "\n",
    "    preds = model(\n",
    "        {\n",
    "            \"user_id\": tf.convert_to_tensor(user_vector, dtype=tf.int32),\n",
    "            \"item_id\": tf.convert_to_tensor(all_items, dtype=tf.int32),\n",
    "        }\n",
    "    ).numpy()\n",
    "\n",
    "    seen_items = user_consumed.get(user_idx, set())\n",
    "    mask = np.ones_like(preds, dtype=bool)\n",
    "    mask[list(seen_items)] = False\n",
    "    filtered_scores = np.where(mask, preds, -np.inf)\n",
    "\n",
    "    top_indices = np.argpartition(filtered_scores, -top_k)[-top_k:]\n",
    "    top_indices = top_indices[np.argsort(filtered_scores[top_indices])[::-1]]\n",
    "\n",
    "    recommendations = []\n",
    "    for item_idx in top_indices:\n",
    "        movie_id = idx_to_movie[item_idx]\n",
    "        recommendations.append(\n",
    "            {\n",
    "                \"movieId\": movie_id,\n",
    "                \"title\": movie_titles.get(movie_id, \"Unknown\"),\n",
    "                \"score\": float(filtered_scores[item_idx]),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(recommendations)\n",
    "\n",
    "\n",
    "sample_user = ratings[\"userId\"].iloc[1]\n",
    "recommend_movies(sample_user)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "092525b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved model weights to saved_model\\model_weights.weights.h5\n",
      "✓ Saved model config to saved_model\\model_config.json\n",
      "✓ Saved mappings to saved_model\\mappings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save model weights and configuration (works with already-trained model)\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "MODEL_DIR = Path(\"saved_model_cf\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "model.save_weights(MODEL_DIR / \"model_weights.weights.h5\")\n",
    "print(f\"✓ Saved model weights to {MODEL_DIR / 'model_weights.weights.h5'}\")\n",
    "\n",
    "# Save model architecture config\n",
    "model_config = {\n",
    "    \"num_users\": num_users,\n",
    "    \"num_items\": num_items,\n",
    "    \"embedding_dim\": 64,\n",
    "    \"reg\": 1e-6,\n",
    "}\n",
    "with open(MODEL_DIR / \"model_config.json\", \"w\") as f:\n",
    "    json.dump(model_config, f)\n",
    "print(f\"✓ Saved model config to {MODEL_DIR / 'model_config.json'}\")\n",
    "\n",
    "# Save mappings for inference\n",
    "mappings = {\n",
    "    \"user_to_idx\": user_to_idx,\n",
    "    \"idx_to_user\": idx_to_user,\n",
    "    \"movie_to_idx\": movie_to_idx,\n",
    "    \"idx_to_movie\": idx_to_movie,\n",
    "    \"movie_titles\": movie_titles,\n",
    "    \"user_consumed\": user_consumed,\n",
    "    \"num_users\": num_users,\n",
    "    \"num_items\": num_items,\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / \"mappings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mappings, f)\n",
    "print(f\"✓ Saved mappings to {MODEL_DIR / 'mappings.pkl'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d803964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved embeddings:\n",
      "  - User embeddings: (138493, 64)\n",
      "  - Item embeddings: (26744, 64)\n",
      "  → Use for approximate nearest neighbor search (FAISS, Annoy, etc.)\n"
     ]
    }
   ],
   "source": [
    "# Export embeddings only (for vector similarity search / ANN systems)\n",
    "# Extract learned embeddings for deployment to vector databases (Pinecone, Weaviate, etc.)\n",
    "\n",
    "user_embeddings = model.user_embedding.get_weights()[\n",
    "    0\n",
    "]  # Shape: (num_users, embedding_dim)\n",
    "item_embeddings = model.item_embedding.get_weights()[\n",
    "    0\n",
    "]  # Shape: (num_items, embedding_dim)\n",
    "user_biases = model.user_bias.get_weights()[0].flatten()\n",
    "item_biases = model.item_bias.get_weights()[0].flatten()\n",
    "\n",
    "np.save(MODEL_DIR / \"user_embeddings.npy\", user_embeddings)\n",
    "np.save(MODEL_DIR / \"item_embeddings.npy\", item_embeddings)\n",
    "np.save(MODEL_DIR / \"user_biases.npy\", user_biases)\n",
    "np.save(MODEL_DIR / \"item_biases.npy\", item_biases)\n",
    "\n",
    "print(\"✓ Saved embeddings:\")\n",
    "print(f\"  - User embeddings: {user_embeddings.shape}\")\n",
    "print(f\"  - Item embeddings: {item_embeddings.shape}\")\n",
    "print(\"  → Use for approximate nearest neighbor search (FAISS, Annoy, etc.)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
